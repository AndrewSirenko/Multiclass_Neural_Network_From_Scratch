{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Computer Vision:  Multi-class neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's start by importing some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make up our 2D data for our three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.zeros((5000, 3)), columns=['x1', 'x2', 'y'])\n",
    "\n",
    "# Let's make up some noisy XOR data to use to build our binary classifier\n",
    "for i in range(len(data.index)):\n",
    "    x1 = random.randint(0,1)\n",
    "    x2 = random.randint(0,1)\n",
    "    if x1 == 1 and x2 == 0:\n",
    "        y = 0\n",
    "    elif x1 == 0 and x2 == 1:\n",
    "        y = 0\n",
    "    elif x1 == 0 and x2 == 0:\n",
    "        y = 1\n",
    "    else:\n",
    "        y = 2\n",
    "    x1 = 1.0 * x1 + 0.20 * np.random.normal()\n",
    "    x2 = 1.0 * x2 + 0.20 * np.random.normal()\n",
    "    data.iloc[i,0] = x1\n",
    "    data.iloc[i,1] = x2\n",
    "    data.iloc[i,2] = y\n",
    "    \n",
    "for i in range(int(0.25 *len(data.index))):\n",
    "    k = np.random.randint(len(data.index)-1)  \n",
    "    data.iloc[k,0] = 1.5 + 0.20 * np.random.normal()\n",
    "    data.iloc[k,1] = 1.5 + 0.20 * np.random.normal()\n",
    "    data.iloc[k,2] = 1\n",
    "\n",
    "for i in range(int(0.25 *len(data.index))):\n",
    "    k = np.random.randint(len(data.index)-1)  \n",
    "    data.iloc[k,0] = 0.5 + 0.20 * np.random.normal()\n",
    "    data.iloc[k,1] = -0.75 + 0.20 * np.random.normal()\n",
    "    data.iloc[k,2] = 2\n",
    "    \n",
    "# Now let's normalize this data.\n",
    "data.iloc[:,0] = (data.iloc[:,0] - data['x1'].mean()) / data['x1'].std()\n",
    "data.iloc[:,1] = (data.iloc[:,1] - data['x2'].mean()) / data['x2'].std()\n",
    "        \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's message this data into a numpy format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set X (training data) and y (target variable)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]\n",
    "\n",
    "# The cost function is expecting numpy matrices so we need to convert X and y before we can use them.  \n",
    "X = np.matrix(X.values)\n",
    "y = np.matrix(y.values)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a sloppy plotting function for our binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sloppy function for plotting our data\n",
    "def plot_data(X, y_predict):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "    indices_0 = [k for k in range(0, X.shape[0]) if y_predict[k] == 0]\n",
    "    indices_1 = [k for k in range(0, X.shape[0]) if y_predict[k] == 1]\n",
    "    indices_2 = [k for k in range(0, X.shape[0]) if y_predict[k] == 2]\n",
    "\n",
    "    ax.plot(X[indices_0, 0], X[indices_0,1], marker='o', linestyle='', ms=5, label='0')\n",
    "    ax.plot(X[indices_1, 0], X[indices_1,1], marker='o', linestyle='', ms=5, label='1')\n",
    "    ax.plot(X[indices_2, 0], X[indices_2,1], marker='o', linestyle='', ms=5, label='2')\n",
    "\n",
    "    ax.legend()\n",
    "    ax.legend(loc=2)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_title('Tricky 3 Class Classification')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Modules from scratch\n",
    "\n",
    "## Layer classes\n",
    "\n",
    "RELU as activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_RATE = 0.01\n",
    "\n",
    "# General template for a layer (forward backward functions + temporary state variables)\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        self.prev = None # previous network (linked list of layers)\n",
    "        self.output = None # output of forward call for backprop.\n",
    "        self.b_vec = None\n",
    "        self.W_mat = None\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backwards(self, *input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# linear (i.e. linear transformation) layer\n",
    "class Linear:\n",
    "    def __init__(self, input_size, output_size, is_input=False):\n",
    "        self.prev = None # previous network (linked list of layers)\n",
    "        self.output = None # output of forward call for backprop.\n",
    "        self.is_input = is_input\n",
    "\n",
    "        self.input = None\n",
    "        \n",
    "        # initialize weights and biases  \n",
    "        # https://www.kdnuggets.com/2018/06/deep-learning-best-practices-weight-initialization.html\n",
    "        # All literature I read said to initiaze biases to 0.1 for optimal performance\n",
    "        # 1, output \n",
    "        self.b_vec = np.zeros((1,output_size)) + 0.5\n",
    "\n",
    "        #* np.sqrt(2 / (input_size + output_size))\n",
    "\n",
    "        # Divide weight initialization by output size in order to help network converge\n",
    "        #    when using MSE as loss function in part E because otherwise weights would get too big\n",
    "        # input, output\n",
    "        he_init = math.sqrt(6/input_size)\n",
    "        #self.W_mat = np.random.uniform(-.25, 0.75, (input_size, output_size)) \n",
    "        self.W_mat = np.random.rand(input_size, output_size)  *2 - 1\n",
    "\n",
    "\n",
    "    def forward(self, input, loss):  # input has shape (batch_size, input_size)\n",
    "        # todo compute forward pass through linear input\n",
    "\n",
    "        if not self.is_input:\n",
    "            input = self.prev.forward(input, loss)\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        output = np.empty((input.shape[0], self.W_mat.shape[1]))\n",
    "\n",
    "        for i, mat in enumerate(input):\n",
    "            output[i] = self.W_mat.T @ mat + self.b_vec\n",
    "\n",
    "        self.output = output\n",
    "        loss.weights_array.append(self.W_mat)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backwards(self, gradient):\n",
    "\n",
    "        dW =  self.input.T @ gradient\n",
    "\n",
    "\n",
    "        dW = np.mean(dW, axis = 0).reshape(1,-1)\n",
    "\n",
    "        # print(\"dW \", dW.shape)\n",
    "        db = np.mean(gradient, axis = 0).reshape(1,-1)\n",
    "        # print(\"db\", db.shape)\n",
    "\n",
    "        new_grad = gradient @ self.W_mat.T\n",
    "\n",
    "\n",
    "        self.W_mat -= L_RATE * dW\n",
    "        self.b_vec -= L_RATE * db\n",
    "        \n",
    "        # print(\"new_grad  \", new_grad.shape)\n",
    "        # print(\"------\")\n",
    "        if self.is_input:\n",
    "            return gradient\n",
    "\n",
    "        return self.prev.backwards(new_grad) \n",
    "\n",
    "# ReLu non-linearity Layer\n",
    "class RELU(Module):\n",
    "    def __init__(self):\n",
    "        super(RELU, self).__init__()\n",
    "\n",
    "    def forward(self, input, loss):\n",
    "        # computes RELU and  updates fields\n",
    "        input = self.prev.forward(input, loss)\n",
    "        self.output = np.maximum(input,0)\n",
    "        return self.output\n",
    "\n",
    "    def backwards(self, gradient):\n",
    "        # computes gradients with backpropogation and data from forward pass\n",
    "        grad_sig = np.where(self.output > 0, 1, 0)\n",
    "\n",
    "\n",
    "        new_gradient = np.multiply(grad_sig,gradient)\n",
    "\n",
    "        return self.prev.backwards(new_gradient)\n",
    "\n",
    "# sigmoid non-linearity TO BE USED AS ACTIVATION FCN\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # todo. compute sigmoid, update fields\n",
    "        input = self.prev.forward(input)\n",
    "        self.output = (1 / (1 + np.exp(-input)))\n",
    "        return self.output\n",
    "\n",
    "    def backwards(self, gradient):\n",
    "        # compute gradients with backpropogation and data from forward pass\n",
    "        grad_sig = self.output * (1 - self.output)\n",
    "\n",
    "        new_gradient = np.multiply(grad_sig,gradient)\n",
    "\n",
    "        # print(\"new_grad_sig \", new_gradient.shape)\n",
    "        # print(\"------\")\n",
    "        return self.prev.backwards(new_gradient)\n",
    "\n",
    "    \n",
    "\n",
    "\"\"\"DEALING WITH LOSS\"\"\"\n",
    "\n",
    "# generic loss layer for loss functions\n",
    "class Loss:\n",
    "    def __init__(self):\n",
    "        self.prev = None\n",
    "\n",
    "    def __call__(self, input):\n",
    "        self.prev = input\n",
    "        return self\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backwards(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SoftMaxLoss(Loss):\n",
    "    def __init__(self):\n",
    "        super(SoftMaxLoss, self).__init__()\n",
    "        self.labels = None\n",
    "        self.output = None\n",
    "        self.mean_loss = None\n",
    "        self.weights_array = list()\n",
    "        self.alpha = 0.0\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        # input has shape (batch_size, input_size)\n",
    "\n",
    "        # Forward pass each input in batch\n",
    "        predictions_batch = self.prev.forward(input, self)\n",
    "        self.labels = labels\n",
    "\n",
    "        predicted_exp = np.exp(predictions_batch)\n",
    "\n",
    "        denom = np.matrix(predicted_exp.sum(1)).T\n",
    "\n",
    "        self.output = (predicted_exp/denom)\n",
    "\n",
    "\n",
    "        z_is = list()\n",
    "        \n",
    "        for i, row in enumerate(predictions_batch):\n",
    "            z_i = np.squeeze(row[labels[i]])\n",
    "\n",
    "            self.output[i,[labels[i]]] -= 1\n",
    "\n",
    "            z_is.append(z_i)\n",
    "        \n",
    "        z_is = np.array(z_is)\n",
    "\n",
    "\n",
    "\n",
    "        # Add regularizatoin by looking at weights matrices of each lin layer\n",
    "        L2_reg = 0\n",
    "        for weight_mat in self.weights_array:\n",
    "            L2_reg += np.linalg.norm(weight_mat) ** 2\n",
    "        \n",
    "        #reset weights array and get L2 Reg\n",
    "        self.weights_array = list()\n",
    "        L2_reg = (self.alpha/2) * L2_reg\n",
    "\n",
    "        # Calculate Total Loss\n",
    "        total_loss = (-z_is + np.log(predicted_exp.sum(1)) + L2_reg)\n",
    "        \n",
    "        # Average Loss over batch size\n",
    "        self.mean_loss = np.average(total_loss, axis=0)\n",
    "        return self.mean_loss\n",
    "\n",
    "\n",
    "    def backwards(self):\n",
    "        # todo compute gradient using backpropogation\n",
    "\n",
    "        # print(\"Loss \", self.output.shape)\n",
    "        return self.prev.backwards(self.output) \n",
    "    \n",
    "    def predict(self, input):\n",
    "        predictions_batch = self.prev.forward(input, self)\n",
    "\n",
    "        predicted_exp = np.exp(predictions_batch)\n",
    "        denom = np.matrix(predicted_exp.sum(1)).T\n",
    "        self.output = (predicted_exp/denom)\n",
    "        return self.output          \n",
    "\n",
    "\n",
    "# Softmax where x_arr is array to softmax on, and y_i \n",
    "def softmax(x_arr, y_i):\n",
    "    x_exp =  np.exp(x_arr)\n",
    "    return (x_exp[:,y_i]/x_exp.sum(1))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP as own class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "def unison_shuffled_copies(a, b):\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "## overall neural network class as asked for in part (a)\n",
    "class MLP:\n",
    "    def __init__(self):\n",
    "        self.l_final = None\n",
    "        self.l_output = None\n",
    "        self.predict = None\n",
    "        self.loss = None\n",
    "        self.weight_mats = None\n",
    "\n",
    "\n",
    "    def add_layer(self, layer_type, dim_in, dim_out):\n",
    "\n",
    "        # Add hidden layer\n",
    "        if layer_type == 'Hidden':\n",
    "            # If first input layer, have some extra logic to make backprop work\n",
    "            if self.l_final is None:\n",
    "                l_input = Linear(dim_in, dim_out, is_input = True)\n",
    "            else:\n",
    "                l_input = Linear(dim_in, dim_out)\n",
    "                l_input.prev = self.l_final\n",
    "\n",
    "            l_hidden = RELU()\n",
    "            l_hidden.prev = l_input\n",
    "            self.l_final = l_hidden \n",
    "\n",
    "        if layer_type == 'Output':\n",
    "            l_final = Linear(dim_in, dim_out)\n",
    "            l_final.prev = self.l_final\n",
    "            self.l_final = l_final\n",
    "\n",
    "        if layer_type == 'Loss':\n",
    "            loss = SoftMaxLoss()\n",
    "            loss.prev = self.l_final\n",
    "            self.loss = loss\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "\n",
    "        mean_loss = self.loss.forward(input, labels)\n",
    "\n",
    "        return mean_loss\n",
    "\n",
    "\n",
    "    def backwards(self):\n",
    "        return self.loss.backwards()\n",
    "\n",
    "    def train(self, data, labels, epochs=100, bsize=8, alpha=0.0, early_stop=False, patience=2):\n",
    "\n",
    "        self.loss.alpha = alpha\n",
    "\n",
    "        data = np.array(data)\n",
    "        labels = labels.astype(int)\n",
    "\n",
    "        batches = math.ceil(data.shape[0] / bsize)\n",
    "\n",
    "        mean_last_epoch_loss = [np.inf]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffling data each epoch shown to increase convergance \n",
    "            shuffled_data, shuffled_labels = unison_shuffled_copies(data, labels)\n",
    "\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for bi in range(batches):\n",
    "                # Gets minibatch\n",
    "                mbidx = bi*bsize\n",
    "                mb_data = shuffled_data[mbidx:mbidx + bsize]\n",
    "\n",
    "\n",
    "                mb_labels = shuffled_labels[mbidx:mbidx + bsize]\n",
    "\n",
    "                # Forwardpass\n",
    "                epoch_loss += self.forward(mb_data, mb_labels)\n",
    "\n",
    "\n",
    "                # Backprop with Adam optimized loss function\n",
    "                self.backwards()\n",
    "            modulo = epochs //5\n",
    "            if modulo == 0:\n",
    "                modulo = 1\n",
    "            if epoch % (modulo) == 0:\n",
    "                print(f\"Epoch: {epoch + 1}/{epochs} -- Mean Loss: {epoch_loss * bsize / data.shape[0]}\")\n",
    "\n",
    "            mean_epoch_loss = epoch_loss * bsize / data.shape[0]\n",
    "\n",
    "\n",
    "            \n",
    "            mean_last_epoch_loss.append(mean_epoch_loss)\n",
    "        return mean_last_epoch_loss\n",
    "\n",
    "    def train_with_val(self, X_train, y_train, X_val, y_val, epochs=100, bsize=8, alpha=0.0, patience=2):\n",
    "        \n",
    "        self.loss.alpha = alpha\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        X_val = np.array(X_val)\n",
    "        y_train = y_train.astype(int)\n",
    "        y_val = y_val.astype(int)\n",
    "\n",
    "        batches = math.ceil(X_train.shape[0] / bsize)\n",
    "        batches_val = math.ceil(X_val.shape[0] / bsize)\n",
    "\n",
    "        mean_last_epoch_loss = [np.inf]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffling data each epoch shown to increase convergance \n",
    "            X_train_shuf, y_train_shuf = unison_shuffled_copies(X_train, y_train)\n",
    "            X_val_shuf, y_val_shuf = unison_shuffled_copies(X_val, y_val)\n",
    "\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for bi in range(batches):\n",
    "                # Gets minibatch\n",
    "                mbidx = bi*bsize\n",
    "                mb_data = X_train_shuf[mbidx:mbidx + bsize]\n",
    "\n",
    "\n",
    "                mb_labels = y_train_shuf[mbidx:mbidx + bsize]\n",
    "\n",
    "                # Forwardpass\n",
    "                self.forward(mb_data, mb_labels)\n",
    "\n",
    "\n",
    "                # Backprop with Adam optimized loss function\n",
    "                self.backwards()\n",
    "\n",
    "            for bi in range(batches_val):\n",
    "                mbidx = bi*bsize\n",
    "                mb_data = X_val_shuf[mbidx:mbidx + bsize]\n",
    "\n",
    "\n",
    "                mb_labels = y_val_shuf[mbidx:mbidx + bsize]\n",
    "\n",
    "                # Forwardpass\n",
    "                epoch_loss += self.forward(mb_data, mb_labels)\n",
    "\n",
    "\n",
    "                # Backprop with Adam optimized loss function\n",
    "                self.backwards()\n",
    "\n",
    "\n",
    "            modulo = epochs //3\n",
    "            if modulo == 0:\n",
    "                modulo = 1\n",
    "            if epoch % (modulo) == 0:\n",
    "                print(f\"Epoch: {epoch + 1}/{epochs} -- Mean Loss: {epoch_loss * bsize / data.shape[0]}\")\n",
    "\n",
    "            mean_epoch_loss = epoch_loss * bsize / data.shape[0]\n",
    "\n",
    "            #Early stopping\n",
    "            if epoch > patience and (mean_epoch_loss > mean_last_epoch_loss[-patience]):\n",
    "                break\n",
    "            \n",
    "            mean_last_epoch_loss.append(mean_epoch_loss)\n",
    "        return mean_last_epoch_loss\n",
    "\n",
    "\n",
    "    def predict_in(self, data):\n",
    "        outputs = self.loss.predict(data)\n",
    "        prediction = np.argmax(outputs, axis=1)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_loss(loss):\n",
    "    x = np.arange(1, len(loss))\n",
    "    y = np.array(loss)[1:]\n",
    "\n",
    "    \n",
    "    # plotting\n",
    "    plt.title(\"Mean Loss by Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Loss\")\n",
    "    plt.plot(x, y, color =\"purple\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_RATE = 0.005\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=1)\n",
    "NN.add_layer('Output', dim_in=1, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=100, bsize=8, alpha=0.0)\n",
    "plot_loss(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision regions of trained classifier for 3 labels (1 hidden unit, 1 hidden layer, regularizatoin = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_decision_regions(NN):\n",
    "    # Densely generate points in plane\n",
    "    axis1 = np.arange(-2.2,2.2, 0.01)\n",
    "    axis2 = np.arange(-2.2,2.2, 0.01)\n",
    "\n",
    "    # Classify points with binary labels\n",
    "    data = np.array(np.meshgrid(axis1, axis2)).T.reshape(-1,2)\n",
    "\n",
    "\n",
    "    predictions = NN.predict_in(data)\n",
    "\n",
    "    \n",
    "    \n",
    "    # PLOT\n",
    "    table = np.concatenate((data,predictions), axis=1)\n",
    "            \n",
    "    df_sampled_points = pd.DataFrame(table, columns=[ 'x1', 'x2', 'label'])\n",
    "\n",
    "    \n",
    "    fig = px.scatter(df_sampled_points, x=\"x1\", y=\"x2\", color=\"label\",\n",
    "                title=f\"Decision region\"\n",
    "                )\n",
    "    fig.update_yaxes(scaleanchor = \"x\",scaleratio = 1,)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 8 hidden units\n",
    "L_RATE = 0.005\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=8)\n",
    "NN.add_layer('Output', dim_in=8, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=200, bsize=20, alpha=0.0)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 16 hidden units\n",
    "L_RATE = 0.005\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=16)\n",
    "NN.add_layer('Output', dim_in=16, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=200, bsize=20, alpha=0.0)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hidden Layer, no regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 hidden units\n",
    "\n",
    "\n",
    "L_RATE = 0.004\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=3)\n",
    "NN.add_layer('Hidden', dim_in=3, dim_out=3)\n",
    "NN.add_layer('Hidden', dim_in=3, dim_out=3)\n",
    "NN.add_layer('Output', dim_in=3, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=200, bsize=20, alpha=0.0)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8 hidden units\n",
    "L_RATE = 0.004\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=8)\n",
    "NN.add_layer('Hidden', dim_in=8, dim_out=8)\n",
    "NN.add_layer('Hidden', dim_in=8, dim_out=8)\n",
    "NN.add_layer('Output', dim_in=8, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=200, bsize=4, alpha=0.0)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 16 hidden units\n",
    "L_RATE = 0.004\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=16)\n",
    "NN.add_layer('Hidden', dim_in=16, dim_out=16)\n",
    "NN.add_layer('Hidden', dim_in=16, dim_out=16)\n",
    "NN.add_layer('Output', dim_in=16, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=150, bsize=8, alpha=0.0)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Hidden Layer, 0.005 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 hidden units\n",
    "L_RATE = 0.002\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=3)\n",
    "NN.add_layer('Output', dim_in=3, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=100, bsize=8, alpha=0.005)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 8 hidden units\n",
    "L_RATE = 0.002\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=8)\n",
    "NN.add_layer('Output', dim_in=8, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=100, bsize=8, alpha=0.005)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 16 hidden units\n",
    "L_RATE = 0.002\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=16)\n",
    "NN.add_layer('Output', dim_in=16, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=50, bsize=8, alpha=0.005)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Hidden Layer, 0.005 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 hidden units\n",
    "L_RATE = 0.002\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=3)\n",
    "NN.add_layer('Hidden', dim_in=3, dim_out=3)\n",
    "NN.add_layer('Hidden', dim_in=3, dim_out=3)\n",
    "NN.add_layer('Output', dim_in=3, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=100, bsize=8, alpha=0.005)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 8 hidden units\n",
    "L_RATE = 0.002\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=8)\n",
    "NN.add_layer('Hidden', dim_in=8, dim_out=8)\n",
    "NN.add_layer('Hidden', dim_in=8, dim_out=8)\n",
    "NN.add_layer('Output', dim_in=8, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=100, bsize=8, alpha=0.004)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 16 hidden units\n",
    "L_RATE = 0.002\n",
    "NN = MLP()\n",
    "NN.add_layer('Hidden', dim_in=2, dim_out=16)\n",
    "NN.add_layer('Hidden', dim_in=16, dim_out=16)\n",
    "NN.add_layer('Hidden', dim_in=16, dim_out=16)\n",
    "NN.add_layer('Output', dim_in=16, dim_out=3)\n",
    "NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = NN.train(X, y, epochs=100, bsize=8, alpha=0.004)\n",
    "plot_loss(loss)\n",
    "plot_decision_regions(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing network on Iris dataset\n",
    "\n",
    "Features: Sepal Length, Sepal Width, and Petal Length\n",
    "\n",
    "CLasses: (3 types of petal) Setosa, Versicolour, and Virginica\n",
    "\n",
    "Link: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X_test, y_test):\n",
    "    predictions = model.predict_in(X_test).squeeze()\n",
    "    \n",
    "    # src: https://stackoverflow.com/questions/20402109/calculating-percentage-error-by-comparing-two-arrays\n",
    "    error = np.mean( predictions != y_test )\n",
    "    return 1 - error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :4]  # First 3 features\n",
    "y = iris.target\n",
    "\n",
    "# Train Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# Train Validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 hidden units\n",
    "L_RATE = 0.0001\n",
    "Custom_NN = MLP()\n",
    "Custom_NN.add_layer('Hidden', dim_in=4, dim_out=16)\n",
    "Custom_NN.add_layer('Hidden', dim_in=16, dim_out=16)\n",
    "Custom_NN.add_layer('Hidden', dim_in=16, dim_out=16)\n",
    "Custom_NN.add_layer('Output', dim_in=16, dim_out=3)\n",
    "Custom_NN.add_layer('Loss', dim_in=3, dim_out=3)\n",
    "\n",
    "loss = Custom_NN.train_with_val(X_train, y_train, X_test, y_test, epochs=200, bsize=1, alpha=0.01, patience = 2)\n",
    "plot_loss(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Custom_NN.predict_in(X_test).squeeze())\n",
    "print(y_test)\n",
    "print(\"\\n\\nAccuracy of model on test data is: \",accuracy(Custom_NN, X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
